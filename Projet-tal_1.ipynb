{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "train = pd.read_excel(\"train.xlsx\")\n",
    "test = pd.read_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickSame(user):\n",
    "    i = random.randint(0,len(train)-1)\n",
    "    tweetUser = train.iloc[i]['user']\n",
    "    cpt =0\n",
    "    while user != tweetUser or cpt > 10000:\n",
    "        cpt += 1\n",
    "        i = random.randint(0,len(train)-1)\n",
    "        tweetUser = train.iloc[i]['user']\n",
    "    return train.iloc[i]['text']\n",
    "def pickDiff(user):\n",
    "    i = random.randint(0,len(train)-1)\n",
    "    tweetUser = train.iloc[i]['user']\n",
    "    cpt =0\n",
    "    while user == tweetUser or cpt > 10000:\n",
    "        cpt += 1\n",
    "        i = random.randint(0,len(train)-1)\n",
    "        tweetUser = train.iloc[i]['user']\n",
    "    return train.iloc[i]['text']\n",
    "def pickRandom(method):\n",
    "    if method == 'train':\n",
    "        i = random.randint(0,len(train)-1)\n",
    "        j = random.randint(0,len(train)-1)\n",
    "        tweet1 = train.iloc[i]['text']\n",
    "        tweet2 = train.iloc[j]['text']\n",
    "    elif method == 'test' :\n",
    "        i = random.randint(0,len(test)-1)\n",
    "        j = random.randint(0,len(test)-1)\n",
    "        tweet1 = test.iloc[i]['text']\n",
    "        tweet2 = test.iloc[j]['text']\n",
    "    return tweet1, tweet2, i ,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.DataFrame(columns=['tweet1', 'tweet2', 'sameUser'])\n",
    "for k in range(int(len(train)/4)):\n",
    "    \n",
    "    tweet1, tweet2 ,i ,j= pickRandom('train')\n",
    "    if train.iloc[i]['user'] == train.iloc[j]['user']:\n",
    "        same_user = 1\n",
    "        tweet3 = pickDiff(train.iloc[i]['user'])\n",
    "        trainData = pd.concat([trainData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet3], 'sameUser': [0]})])\n",
    "    else:\n",
    "        same_user = 0\n",
    "        tweet3 = pickSame(train.iloc[i]['user'])\n",
    "        trainData = pd.concat([trainData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet3], 'sameUser': [1]})])\n",
    "    trainData = pd.concat([trainData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet2], 'sameUser': [same_user]})])\n",
    "\n",
    "testData = pd.DataFrame(columns=['tweet1', 'tweet2', 'sameUser'])\n",
    "for k in range(int(len(test)/2)):\n",
    "    tweet1, tweet2 ,i ,j= pickRandom('test')\n",
    "    if test.iloc[i]['user'] == test.iloc[j]['user']:\n",
    "        same_user = 1\n",
    "        tweet3 = pickDiff(train.iloc[i]['user'])\n",
    "        testData = pd.concat([testData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet3], 'sameUser': [0]})])\n",
    "    else:\n",
    "        same_user = 0\n",
    "        tweet3 = pickSame(train.iloc[i]['user'])\n",
    "        testData = pd.concat([testData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet3], 'sameUser': [1]})])\n",
    "    testData = pd.concat([testData, pd.DataFrame({'tweet1': [tweet1], 'tweet2': [tweet2], 'sameUser': [same_user]})])\n",
    "\n",
    "\n",
    "# nradohom gir horof o nkhaliw hachtag o arobaze\n",
    "testData['tweet1'] = testData['tweet1'].apply(remove_special_chars)\n",
    "#nradohom lower case \n",
    "testData['tweet1'] = testData['tweet1'].str.lower()\n",
    "# nradohom gir horof o nkhaliw hachtag o arobaze\n",
    "testData['tweet2'] = testData['tweet2'].apply(remove_special_chars)\n",
    "#nradohom lower case \n",
    "testData['tweet2'] = testData['tweet2'].str.lower()\n",
    "\n",
    "\n",
    "# nradohom gir horof o nkhaliw hachtag o arobaze\n",
    "trainData['tweet1'] = trainData['tweet1'].apply(remove_special_chars)\n",
    "#nradohom lower case \n",
    "trainData['tweet1'] = trainData['tweet1'].str.lower()\n",
    "# nradohom gir horof o nkhaliw hachtag o arobaze\n",
    "trainData['tweet2'] = trainData['tweet2'].apply(remove_special_chars)\n",
    "#nradohom lower case \n",
    "trainData['tweet2'] = trainData['tweet2'].str.lower()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet1</th>\n",
       "      <th>tweet2</th>\n",
       "      <th>sameUser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foreign policy leaders explain why president o...</td>\n",
       "      <td>go behind the scenes on the campaign trail on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foreign policy leaders explain why president o...</td>\n",
       "      <td>2 pockets</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>major story that the dems are making up phony ...</td>\n",
       "      <td>economic confidence is soaring as we unleash t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>major story that the dems are making up phony ...</td>\n",
       "      <td>#happyfathersday mom i love u and thank u</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proud of my girl @tamarbraxtonher her album lo...</td>\n",
       "      <td>#18daystillartpop look at me now i feel on top...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>join me on @greta from indianapolis indiana at...</td>\n",
       "      <td>heres my view right now rt @portiaderossi you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@bieberarmy #fanmade lyric vid nice thanks for...</td>\n",
       "      <td>going to stay in the top 3 on billboard this w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@bieberarmy #fanmade lyric vid nice thanks for...</td>\n",
       "      <td>sundays top 5 plays #nbaplayoffspictwittercomk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oy aussies you made my summer song the #1 song...</td>\n",
       "      <td>get a whiff of this my new fragrance is coming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oy aussies you made my summer song the #1 song...</td>\n",
       "      <td>#believemovie httpsitunesapplecomusmoviejustin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet1  \\\n",
       "0   foreign policy leaders explain why president o...   \n",
       "0   foreign policy leaders explain why president o...   \n",
       "0   major story that the dems are making up phony ...   \n",
       "0   major story that the dems are making up phony ...   \n",
       "0   proud of my girl @tamarbraxtonher her album lo...   \n",
       "..                                                ...   \n",
       "0   join me on @greta from indianapolis indiana at...   \n",
       "0   @bieberarmy #fanmade lyric vid nice thanks for...   \n",
       "0   @bieberarmy #fanmade lyric vid nice thanks for...   \n",
       "0   oy aussies you made my summer song the #1 song...   \n",
       "0   oy aussies you made my summer song the #1 song...   \n",
       "\n",
       "                                               tweet2 sameUser  \n",
       "0   go behind the scenes on the campaign trail on ...        1  \n",
       "0                                           2 pockets        0  \n",
       "0   economic confidence is soaring as we unleash t...        1  \n",
       "0           #happyfathersday mom i love u and thank u        0  \n",
       "0   #18daystillartpop look at me now i feel on top...        1  \n",
       "..                                                ...      ...  \n",
       "0   heres my view right now rt @portiaderossi you ...        0  \n",
       "0   going to stay in the top 3 on billboard this w...        1  \n",
       "0   sundays top 5 plays #nbaplayoffspictwittercomk...        0  \n",
       "0   get a whiff of this my new fragrance is coming...        1  \n",
       "0   #believemovie httpsitunesapplecomusmoviejustin...        0  \n",
       "\n",
       "[13000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer(text1, text2,model,tokenizer):\n",
    "\n",
    "\n",
    "    encoded_input1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    encoded_input2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**encoded_input1)\n",
    "        outputs2 = model(**encoded_input2)\n",
    "\n",
    "    sentence_representation1 = outputs1.last_hidden_state.mean(dim=1)\n",
    "    sentence_representation2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "    MDist = cityblock(sentence_representation1, sentence_representation2)\n",
    "    return MDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet1</th>\n",
       "      <th>tweet2</th>\n",
       "      <th>sameUser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foreign policy leaders explain why president o...</td>\n",
       "      <td>go behind the scenes on the campaign trail on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>major story that the dems are making up phony ...</td>\n",
       "      <td>economic confidence is soaring as we unleash t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proud of my girl @tamarbraxtonher her album lo...</td>\n",
       "      <td>#18daystillartpop look at me now i feel on top...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tesla dual motor cars are also allwheel drive ...</td>\n",
       "      <td>right question to ask dyson or others what is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when dani learned what genderqueer meant it wa...</td>\n",
       "      <td>@asmnzf we see what you did there and we like it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go guys rt @vivaronaldo lets vote #vivaronaldo...</td>\n",
       "      <td>ontem conseguirmos uma importante vitÃ³ria para...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great show again in sydney love australia than...</td>\n",
       "      <td>httpyoutubelujn3rpkcky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>join me on @greta from indianapolis indiana at...</td>\n",
       "      <td>stock market hit another alltime high yesterda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@bieberarmy #fanmade lyric vid nice thanks for...</td>\n",
       "      <td>going to stay in the top 3 on billboard this w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oy aussies you made my summer song the #1 song...</td>\n",
       "      <td>get a whiff of this my new fragrance is coming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet1  \\\n",
       "0   foreign policy leaders explain why president o...   \n",
       "0   major story that the dems are making up phony ...   \n",
       "0   proud of my girl @tamarbraxtonher her album lo...   \n",
       "0   tesla dual motor cars are also allwheel drive ...   \n",
       "0   when dani learned what genderqueer meant it wa...   \n",
       "..                                                ...   \n",
       "0   go guys rt @vivaronaldo lets vote #vivaronaldo...   \n",
       "0   great show again in sydney love australia than...   \n",
       "0   join me on @greta from indianapolis indiana at...   \n",
       "0   @bieberarmy #fanmade lyric vid nice thanks for...   \n",
       "0   oy aussies you made my summer song the #1 song...   \n",
       "\n",
       "                                               tweet2 sameUser  \n",
       "0   go behind the scenes on the campaign trail on ...        1  \n",
       "0   economic confidence is soaring as we unleash t...        1  \n",
       "0   #18daystillartpop look at me now i feel on top...        1  \n",
       "0   right question to ask dyson or others what is ...        1  \n",
       "0    @asmnzf we see what you did there and we like it        1  \n",
       "..                                                ...      ...  \n",
       "0   ontem conseguirmos uma importante vitÃ³ria para...        1  \n",
       "0                             httpyoutubelujn3rpkckyÂ         1  \n",
       "0   stock market hit another alltime high yesterda...        1  \n",
       "0   going to stay in the top 3 on billboard this w...        1  \n",
       "0   get a whiff of this my new fragrance is coming...        1  \n",
       "\n",
       "[6500 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[trainData['sameUser'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.5149, Accuracy : 0.47474747474747475\n",
      "Epoch [200/10000], Loss: 1.3169, Accuracy : 0.5175879396984925\n",
      "Epoch [300/10000], Loss: 0.6689, Accuracy : 0.5016722408026756\n",
      "Epoch [400/10000], Loss: 0.7965, Accuracy : 0.5012531328320802\n",
      "Epoch [500/10000], Loss: 1.0124, Accuracy : 0.501002004008016\n",
      "Epoch [600/10000], Loss: 0.8256, Accuracy : 0.5075125208681135\n",
      "Epoch [700/10000], Loss: 0.6749, Accuracy : 0.5064377682403434\n",
      "Epoch [800/10000], Loss: 0.7238, Accuracy : 0.5031289111389237\n",
      "Epoch [900/10000], Loss: 0.5448, Accuracy : 0.5072302558398221\n",
      "Epoch [1000/10000], Loss: 0.6489, Accuracy : 0.5035035035035035\n",
      "Epoch [1100/10000], Loss: 1.0792, Accuracy : 0.49954504094631486\n",
      "Epoch [1200/10000], Loss: 0.2975, Accuracy : 0.5054211843202668\n",
      "Epoch [1300/10000], Loss: 0.9099, Accuracy : 0.5065434949961509\n",
      "Epoch [1400/10000], Loss: 0.4052, Accuracy : 0.5110793423874196\n",
      "Epoch [1500/10000], Loss: 1.1399, Accuracy : 0.5063375583722481\n",
      "Epoch [1600/10000], Loss: 1.0331, Accuracy : 0.5053158223889931\n",
      "Epoch [1700/10000], Loss: 0.7588, Accuracy : 0.5044143613890524\n",
      "Epoch [1800/10000], Loss: 0.8542, Accuracy : 0.5013896609227348\n",
      "Epoch [1900/10000], Loss: 0.5359, Accuracy : 0.5023696682464455\n",
      "Epoch [2000/10000], Loss: 0.8352, Accuracy : 0.5022511255627814\n",
      "Epoch [2100/10000], Loss: 0.4730, Accuracy : 0.5021438780371605\n",
      "Epoch [2200/10000], Loss: 0.2214, Accuracy : 0.5052296498408367\n",
      "Epoch [2300/10000], Loss: 1.0486, Accuracy : 0.5032622879512831\n",
      "Epoch [2400/10000], Loss: 0.6522, Accuracy : 0.5002084201750729\n",
      "Epoch [2500/10000], Loss: 0.8833, Accuracy : 0.501000400160064\n",
      "Epoch [2600/10000], Loss: 0.6667, Accuracy : 0.5013466717968449\n",
      "Epoch [2700/10000], Loss: 1.0435, Accuracy : 0.49981474620229716\n",
      "Epoch [2800/10000], Loss: 0.9628, Accuracy : 0.5016077170418006\n",
      "Epoch [2900/10000], Loss: 0.7636, Accuracy : 0.49913763366678165\n",
      "Epoch [3000/10000], Loss: 0.7181, Accuracy : 0.49716572190730246\n",
      "Epoch [3100/10000], Loss: 0.5847, Accuracy : 0.49499838657631495\n",
      "Epoch [3200/10000], Loss: 0.5646, Accuracy : 0.4954673335417318\n",
      "Epoch [3300/10000], Loss: 0.4472, Accuracy : 0.49651409518035766\n",
      "Epoch [3400/10000], Loss: 0.6626, Accuracy : 0.4954398352456605\n",
      "Epoch [3500/10000], Loss: 0.8033, Accuracy : 0.4967133466704773\n",
      "Epoch [3600/10000], Loss: 0.5151, Accuracy : 0.4979160878021673\n",
      "Epoch [3700/10000], Loss: 0.6809, Accuracy : 0.4985131116517978\n",
      "Epoch [3800/10000], Loss: 0.5947, Accuracy : 0.4985522505922611\n",
      "Epoch [3900/10000], Loss: 0.9248, Accuracy : 0.500128238009746\n",
      "Epoch [4000/10000], Loss: 1.1891, Accuracy : 0.49987496874218557\n",
      "Epoch [4100/10000], Loss: 0.6133, Accuracy : 0.49987801902903145\n",
      "Epoch [4200/10000], Loss: 0.7399, Accuracy : 0.4994046201476542\n",
      "Epoch [4300/10000], Loss: 0.9316, Accuracy : 0.5008141428239126\n",
      "Epoch [4400/10000], Loss: 0.7419, Accuracy : 0.5001136621959537\n",
      "Epoch [4500/10000], Loss: 0.9268, Accuracy : 0.49877750611246946\n",
      "Epoch [4600/10000], Loss: 0.4948, Accuracy : 0.4968471406827571\n",
      "Epoch [4700/10000], Loss: 0.8551, Accuracy : 0.49478612470738453\n",
      "Epoch [4800/10000], Loss: 0.6198, Accuracy : 0.49468639299854134\n",
      "Epoch [4900/10000], Loss: 0.9135, Accuracy : 0.4927536231884058\n",
      "Epoch [5000/10000], Loss: 0.7582, Accuracy : 0.49249849969994\n",
      "Epoch [5100/10000], Loss: 0.6639, Accuracy : 0.49244949990194153\n",
      "Epoch [5200/10000], Loss: 0.6351, Accuracy : 0.4933641084824005\n",
      "Epoch [5300/10000], Loss: 1.0385, Accuracy : 0.4949990564257407\n",
      "Epoch [5400/10000], Loss: 0.5923, Accuracy : 0.49675865901092797\n",
      "Epoch [5500/10000], Loss: 1.4683, Accuracy : 0.49681760320058194\n",
      "Epoch [5600/10000], Loss: 0.4348, Accuracy : 0.49741025183068405\n",
      "Epoch [5700/10000], Loss: 1.3652, Accuracy : 0.49850851026495874\n",
      "Epoch [5800/10000], Loss: 0.3193, Accuracy : 0.4980168994654251\n",
      "Epoch [5900/10000], Loss: 0.4341, Accuracy : 0.4978809967791151\n",
      "Epoch [6000/10000], Loss: 1.0332, Accuracy : 0.49658276379396565\n",
      "Epoch [6100/10000], Loss: 1.1405, Accuracy : 0.49647483193966224\n",
      "Epoch [6200/10000], Loss: 0.5316, Accuracy : 0.49669301500241975\n",
      "Epoch [6300/10000], Loss: 0.5022, Accuracy : 0.49642800444515\n",
      "Epoch [6400/10000], Loss: 0.7639, Accuracy : 0.4964838255977497\n",
      "Epoch [6500/10000], Loss: 0.7583, Accuracy : 0.4974611478689029\n",
      "Epoch [6600/10000], Loss: 0.3172, Accuracy : 0.4985603879375663\n",
      "Epoch [6700/10000], Loss: 0.5453, Accuracy : 0.4993282579489476\n",
      "Epoch [6800/10000], Loss: 0.6651, Accuracy : 0.4999264597734961\n",
      "Epoch [6900/10000], Loss: 0.8113, Accuracy : 0.5000724742716336\n",
      "Epoch [7000/10000], Loss: 0.5618, Accuracy : 0.500500071438777\n",
      "Epoch [7100/10000], Loss: 0.7166, Accuracy : 0.5000704324552754\n",
      "Epoch [7200/10000], Loss: 0.7671, Accuracy : 0.5003472704542298\n",
      "Epoch [7300/10000], Loss: 0.4150, Accuracy : 0.5007535278805316\n",
      "Epoch [7400/10000], Loss: 0.9139, Accuracy : 0.5011488038924179\n",
      "Epoch [7500/10000], Loss: 0.4782, Accuracy : 0.5022002933724496\n",
      "Epoch [7600/10000], Loss: 0.6196, Accuracy : 0.502829319647322\n",
      "Epoch [7700/10000], Loss: 0.9789, Accuracy : 0.5027925704636966\n",
      "Epoch [7800/10000], Loss: 0.7503, Accuracy : 0.5022438774201821\n",
      "Epoch [7900/10000], Loss: 1.1119, Accuracy : 0.5023420686162805\n",
      "Epoch [8000/10000], Loss: 0.4955, Accuracy : 0.502187773471684\n",
      "Epoch [8100/10000], Loss: 0.9293, Accuracy : 0.5016668724533893\n",
      "Epoch [8200/10000], Loss: 0.7857, Accuracy : 0.5025003049152336\n",
      "Epoch [8300/10000], Loss: 1.0422, Accuracy : 0.5035546451379684\n",
      "Epoch [8400/10000], Loss: 0.9542, Accuracy : 0.5035123228955828\n",
      "Epoch [8500/10000], Loss: 0.6533, Accuracy : 0.5030003529827038\n",
      "Epoch [8600/10000], Loss: 0.3935, Accuracy : 0.5034306314687754\n",
      "Epoch [8700/10000], Loss: 0.6832, Accuracy : 0.5031612829060812\n",
      "Epoch [8800/10000], Loss: 0.8832, Accuracy : 0.5035799522673031\n",
      "Epoch [8900/10000], Loss: 0.8292, Accuracy : 0.504438700977638\n",
      "Epoch [9000/10000], Loss: 0.3739, Accuracy : 0.5047227469718858\n",
      "Epoch [9100/10000], Loss: 0.5385, Accuracy : 0.5046708429497747\n",
      "Epoch [9200/10000], Loss: 0.6694, Accuracy : 0.5046200673986303\n",
      "Epoch [9300/10000], Loss: 0.5374, Accuracy : 0.505000537692225\n",
      "Epoch [9400/10000], Loss: 0.6585, Accuracy : 0.5036706032556655\n",
      "Epoch [9500/10000], Loss: 0.7113, Accuracy : 0.5036319612590799\n",
      "Epoch [9600/10000], Loss: 0.7119, Accuracy : 0.5036983019064486\n",
      "Epoch [9700/10000], Loss: 0.6334, Accuracy : 0.5033508609134962\n",
      "Epoch [9800/10000], Loss: 0.3885, Accuracy : 0.5037248698846821\n",
      "Epoch [9900/10000], Loss: 0.7175, Accuracy : 0.5038892817456309\n",
      "Epoch [10000/10000], Loss: 0.5489, Accuracy : 0.504050405040504\n"
     ]
    }
   ],
   "source": [
    "dense_layer = nn.Sequential(\n",
    "    nn.Linear(1, 50),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(50, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(dense_layer.parameters(), lr=0.007)\n",
    "accuracyCpt = 0\n",
    "cpt3 =0\n",
    "cpt2 = 0\n",
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "for step in range(10000):\n",
    "    randomTweet = random.randint(0,len(trainData)-1)\n",
    "    x1= trainData.iloc[randomTweet]['tweet1']\n",
    "    x2= trainData.iloc[randomTweet]['tweet2']\n",
    "    res = trainData.iloc[randomTweet]['sameUser']\n",
    "    \n",
    "    distance = Transformer(x1, x2,model,tokenizer)\n",
    "    pred = dense_layer(torch.Tensor([distance]))\n",
    "    loss = loss_function(pred, torch.Tensor([res]))\n",
    "    if round(pred.item()) == res:\n",
    "        accuracyCpt +=1\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(round(pred.item()) == 1):\n",
    "        cpt2 += 1\n",
    "    else:\n",
    "        cpt3 += 1\n",
    "    # Print loss\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f'Epoch [{step+1}/{10000}], Loss: {loss.item():.4f}, Accuracy : {accuracyCpt/step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for idx in range(len(testData)):\n",
    "    x1 = testData.iloc[idx]['tweet1']\n",
    "    x2 = testData.iloc[idx]['tweet2']\n",
    "    true_label = testData.iloc[idx]['sameUser']\n",
    "    \n",
    "    distance = Transformer(x1, x2, model, tokenizer)\n",
    "\n",
    "    pred = dense_layer(torch.Tensor([distance]))\n",
    "    \n",
    "    predicted_label = round(pred.item())\n",
    "\n",
    "    true_labels.append(true_label)\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
